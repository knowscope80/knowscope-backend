# ğŸ“š Knowscope Content Service â€” RAG Textbook Q&A

A fully automated **Retrieval-Augmented Generation (RAG)** system that lets students ask questions from their textbooks and receive accurate, exam-style answers â€” automatically.

```
Teacher uploads PDF  â†’  System processes & indexes textbook
Student asks question  â†’  System retrieves relevant content  â†’  GPT generates answer
```

---

## ğŸ—ï¸ Architecture

```
PDF Upload â”€â”€â–º pdf_loader â”€â”€â–º text_cleaner â”€â”€â–º chapter_pipeline
                                                      â”‚
                                               topic_extractor
                                                      â”‚
                                               chunk_builder â”€â”€â–º embedding_service
                                                                        â”‚
                                                                   ChromaDB (vector DB)
                                                                        â”‚
Student Question â”€â”€â–º LangGraph RAG Graph â”€â”€â–º embed_question
                                                  â”‚
                                            retrieve_chunks  â†â”€â”€ ChromaDB
                                                  â”‚
                                            generate_answer  â”€â”€â–º OpenAI GPT
                                                  â”‚
                                          Exam-style Answer â”€â”€â–º Student
```

**Stack:**
| Component | Technology |
|---|---|
| Backend API | FastAPI |
| Vector Database | ChromaDB (persistent, local) |
| Embeddings | SentenceTransformers (`BAAI/bge-small-en-v1.5`) |
| RAG Orchestration | LangGraph |
| LLM | OpenAI GPT-3.5-Turbo |
| PDF Parsing | pdfplumber |
| Document Store | MongoDB (Atlas) |

---

## âš™ï¸ Setup

### 1. Prerequisites
- Python 3.10+
- MongoDB Atlas cluster (URI in `.env`)
- OpenAI API key (optional â€” fallback returns raw text chunks)

### 2. Install Dependencies

```powershell
cd backend\content_service
python -m venv content_venv
.\content_venv\Scripts\activate
pip install -r requirements.txt
```

### 3. Configure Environment

Edit `.env`:
```env
MONGO_URI=mongodb+srv://<user>:<password>@<cluster>.mongodb.net/?appName=knowscope
MONGO_DB=knowscope
OPENAI_API_KEY=sk-...your-real-key-here...
```

> **Note:** Without `OPENAI_API_KEY`, the system falls back to returning the most relevant raw textbook passages.

### 4. Run the Server

```powershell
cd backend\content_service
.\content_venv\Scripts\activate
uvicorn app.main:app --reload --port 8001
```

API docs available at: **http://localhost:8001/docs**

---

## ğŸ“¡ API Reference

### ğŸ“¤ Ingestion (Teacher / Admin)

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/ingest/pdf` | Upload a PDF textbook and index it |
| `GET` | `/ingest/books` | List all ingested books |
| `DELETE` | `/ingest/book/{book_id}` | Remove a book from all stores |

#### Upload PDF
```bash
curl -X POST http://localhost:8001/ingest/pdf \
  -F "file=@textbook.pdf" \
  -F "book_id=biology_class10" \
  -F "class_number=10" \
  -F "subject=biology"
```

Response:
```json
{
  "message": "âœ… PDF ingested successfully",
  "book_id": "biology_class10",
  "pages_extracted": 248,
  "chapters_created": 12,
  "total_chunks_indexed": 387
}
```

---

### â“ Question Answering (Student)

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/qa/ask` | Ask a question â€” full RAG answer |
| `POST` | `/api/qa/search` | Retrieve raw chunks (no LLM) |
| `GET` | `/api/qa/stats` | ChromaDB statistics |
| `GET` | `/api/qa/books` | List indexed books in ChromaDB |
| `DELETE` | `/api/qa/book/{book_id}` | Remove book vectors |

#### Ask a Question (Student provides ONLY the question)
```bash
curl -X POST http://localhost:8001/api/qa/ask \
  -H "Content-Type: application/json" \
  -d '{"question": "Explain photosynthesis."}'
```

Response:
```json
{
  "answer": "Photosynthesis is the process by which green plants use sunlight...",
  "sources": [
    {
      "chapter": "Life Processes",
      "topic": "Photosynthesis",
      "similarity": 0.923,
      "text_preview": "Photosynthesis is the process by which..."
    }
  ],
  "confidence": 0.887,
  "total_chunks_used": 5
}
```

---

## ğŸ§ª Testing

### Verify all imports
```powershell
python scripts/verify_setup.py
```

### End-to-end RAG pipeline test
```powershell
python scripts/test_qa.py
```
This seeds 3 photosynthesis chunks, runs embedding â†’ search â†’ GPT answer â†’ cleanup.

---

## ğŸ“‚ Project Structure

```
content_service/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py            # FastAPI app + lifespan
â”‚   â”œâ”€â”€ database.py        # MongoDB (Motor) collections
â”‚   â”œâ”€â”€ vector_store.py    # ChromaDB wrapper
â”‚   â””â”€â”€ models.py          # Pydantic models
â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ ingest.py          # PDF upload endpoints
â”‚   â””â”€â”€ qa.py              # Q&A endpoints
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ pdf_loader.py      # PDF text extraction
â”‚   â”œâ”€â”€ chapter_pipeline.py # TOC-based chapter splitting
â”‚   â”œâ”€â”€ topic_extractor.py  # Topic segmentation
â”‚   â”œâ”€â”€ chunk_builder.py    # Chunking + embedding + storage
â”‚   â”œâ”€â”€ embedding_service.py # SentenceTransformer wrapper
â”‚   â”œâ”€â”€ rag_graph.py        # LangGraph RAG pipeline
â”‚   â””â”€â”€ gpt_service.py      # OpenAI GPT answer generation
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ text_cleaner.py     # Generic PDF text cleaning
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ test_qa.py          # End-to-end RAG test
â”‚   â””â”€â”€ verify_setup.py     # Import verification
â”œâ”€â”€ chroma_db_data/         # ChromaDB persistent storage (auto-created)
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env
```

---

## ğŸ’¡ How It Works

### PDF Ingestion
1. PDF text is extracted page-by-page using `pdfplumber`
2. Text is cleaned (page numbers, artifacts, repeated headers removed)
3. Table of Contents is detected and used to split into chapters
4. Chapters are further segmented by topic using paragraph detection
5. Each topic is split into 400-word chunks with 50-word overlap
6. Chunks are embedded using `BAAI/bge-small-en-v1.5` (384-dim vectors)
7. Vectors are stored in ChromaDB; raw text is stored in MongoDB

### Student Q&A (RAG Pipeline)
1. Student question is embedded with the same model
2. ChromaDB performs cosine similarity search across **all** stored textbooks
3. Top-K most relevant chunks are retrieved as context
4. GPT-3.5-Turbo generates a structured exam-style answer based **only** on retrieved context
5. If no relevant content found (confidence < 0.25), returns `"No answer found in textbook."`
